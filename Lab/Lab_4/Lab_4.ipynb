{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: 0\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "plt.imshow(np.reshape(digits.data[0], (8, 8)), cmap='gray')\n",
    "plt.title('Label: %i\\n' % digits.target[0], fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(digits.data,train_size = 0.75, shuffle = False)\n",
    "train_label, test_label = train_test_split(digits.target,train_size = 0.75, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMP 9517 Week 5 Lab - z5193723\n",
      "\n",
      "Test size = 0.25\n",
      "KNN Accuracy:     0.971   recall: 0.971\n",
      "SGD Accuracy:     0.900   recall: 0.900\n",
      "DT Accuracy:      0.791   recall: 0.791\n",
      "\n",
      "KNN Confusion Matrix:\n",
      "[[43  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 46  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 43  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 43  0  1  0  1  1  0]\n",
      " [ 0  0  0  0 45  0  0  1  1  1]\n",
      " [ 0  0  0  0  0 44  1  0  0  0]\n",
      " [ 0  0  0  0  0  0 47  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 45  0  0]\n",
      " [ 0  2  0  0  0  0  0  0 39  0]\n",
      " [ 0  0  0  1  0  2  0  0  0 42]]\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(3)\n",
    "KNN.fit(train_set,train_label)\n",
    "KNN_prediction = KNN.predict(test_set)\n",
    "\n",
    "KNN_accuracy = metrics.accuracy_score(test_label,KNN_prediction)\n",
    "KNN_recall = metrics.recall_score(test_label,KNN_prediction,average= 'micro')\n",
    "\n",
    "# print(accuracy)\n",
    "# print(metrics.confusion_matrix(test_label,prediction))\n",
    "# print(recall)\n",
    "\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "DTC = DecisionTreeClassifier(criterion='entropy',max_depth = 12)\n",
    "DTC.fit(train_set,train_label)\n",
    "DTC_prediction = DTC.predict(test_set)\n",
    "\n",
    "DTC_accuracy = metrics.accuracy_score(test_label,DTC_prediction)\n",
    "DTC_recall = metrics.recall_score(test_label,DTC_prediction,average= 'micro')\n",
    "\n",
    "\n",
    "#SGDClassifier\n",
    "SGD=SGDClassifier(loss=\"log\",penalty=\"l2\")\n",
    "SGD.fit(train_set,train_label)\n",
    "SGD_pre = SGD.predict(test_set)\n",
    "\n",
    "SGD_accuracy = metrics.accuracy_score(test_label,SGD_pre)\n",
    "SGD_recall = metrics.recall_score(test_label,SGD_pre,average= 'micro')\n",
    "\n",
    "\n",
    "# print(classification_report(test_label, SGD_pre))\n",
    "\n",
    "print(\"COMP 9517 Week 5 Lab - z5193723\")\n",
    "print()\n",
    "print(\"Test size = 0.25\")\n",
    "print(\"KNN Accuracy:%10.3f\"%KNN_accuracy+\"   \"+\"recall:%6.3f\"%KNN_recall)\n",
    "print(\"SGD Accuracy:%10.3f\"%SGD_accuracy+\"   \"+\"recall:%6.3f\"%SGD_recall)\n",
    "print(\"DT Accuracy:%11.3f\"%DTC_accuracy+\"   \"+\"recall:%6.3f\"%DTC_recall)\n",
    "print()\n",
    "print(\"KNN Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(test_label,KNN_prediction))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450,) (450,)\n"
     ]
    }
   ],
   "source": [
    "print(test_label.shape,KNN_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier(criterion='entropy')\n",
    "DTC.fit(train_set,train_label)\n",
    "DTC_prediction = KNN.predict(test_set)\n",
    "\n",
    "accuracy = metrics.accuracy_score(test_label,DTC_prediction)\n",
    "recall = metrics.recall_score(test_label,DTC_prediction,average= 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111111111111111\n",
      "0.9117522351470745\n"
     ]
    }
   ],
   "source": [
    "SGD=SGDClassifier(loss=\"log\",penalty=\"l2\")\n",
    "SGD.fit(train_set,train_label)\n",
    "SGD_pre = SGD.predict(test_set)\n",
    "\n",
    "accuracy = metrics.accuracy_score(test_label,SGD_pre)\n",
    "recall = metrics.recall_score(test_label,SGD_pre,average= 'macro')\n",
    "\n",
    "print(accuracy)\n",
    "print(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function recall_score in module sklearn.metrics.classification:\n",
      "\n",
      "recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "    Compute the recall\n",
      "    \n",
      "    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "    true positives and ``fn`` the number of false negatives. The recall is\n",
      "    intuitively the ability of the classifier to find all the positive samples.\n",
      "    \n",
      "    The best value is 1 and the worst value is 0.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : list, optional\n",
      "        The set of labels to include when ``average != 'binary'``, and their\n",
      "        order if ``average is None``. Labels present in the data can be\n",
      "        excluded, for example to calculate a multiclass average ignoring a\n",
      "        majority negative class, while labels not present in the data will\n",
      "        result in 0 components in a macro average. For multilabel targets,\n",
      "        labels are column indices. By default, all labels in ``y_true`` and\n",
      "        ``y_pred`` are used in sorted order.\n",
      "    \n",
      "        .. versionchanged:: 0.17\n",
      "           parameter *labels* improved for multiclass problem.\n",
      "    \n",
      "    pos_label : str or int, 1 by default\n",
      "        The class to report if ``average='binary'`` and the data is binary.\n",
      "        If the data are multiclass or multilabel, this will be ignored;\n",
      "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "        scores for that label only.\n",
      "    \n",
      "    average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "        This parameter is required for multiclass/multilabel targets.\n",
      "        If ``None``, the scores for each class are returned. Otherwise, this\n",
      "        determines the type of averaging performed on the data:\n",
      "    \n",
      "        ``'binary'``:\n",
      "            Only report results for the class specified by ``pos_label``.\n",
      "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "        ``'micro'``:\n",
      "            Calculate metrics globally by counting the total true positives,\n",
      "            false negatives and false positives.\n",
      "        ``'macro'``:\n",
      "            Calculate metrics for each label, and find their unweighted\n",
      "            mean.  This does not take label imbalance into account.\n",
      "        ``'weighted'``:\n",
      "            Calculate metrics for each label, and find their average weighted\n",
      "            by support (the number of true instances for each label). This\n",
      "            alters 'macro' to account for label imbalance; it can result in an\n",
      "            F-score that is not between precision and recall.\n",
      "        ``'samples'``:\n",
      "            Calculate metrics for each instance, and find their average (only\n",
      "            meaningful for multilabel classification where this differs from\n",
      "            :func:`accuracy_score`).\n",
      "    \n",
      "    sample_weight : array-like of shape = [n_samples], optional\n",
      "        Sample weights.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    recall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        Recall of the positive class in binary classification or weighted\n",
      "        average of the recall of each class for the multiclass task.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    precision_recall_fscore_support, balanced_accuracy_score,\n",
      "    multilabel_confusion_matrix\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import recall_score\n",
      "    >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "    >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "    >>> recall_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "    0.33...\n",
      "    >>> recall_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "    0.33...\n",
      "    >>> recall_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
      "    0.33...\n",
      "    >>> recall_score(y_true, y_pred, average=None)\n",
      "    array([1., 0., 0.])\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "    ``UndefinedMetricWarning``.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(metrics.recall_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6153846153846154"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= 1\n",
    "b = rec(a,5)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 8, 16, 32] [3, 6, 12, 24, 48, 0]\n"
     ]
    }
   ],
   "source": [
    "a = [0,0,0,0,0,0]\n",
    "\n",
    "b = [0,0,0,0,0,0]\n",
    "\n",
    "def func(a,b,i):\n",
    "    if i > 0:\n",
    "        y= 5*func(a,b,i - 1) - fung(a,b,i-1) \n",
    "    else:\n",
    "        y = 1\n",
    "    a[i] = y\n",
    "    return y\n",
    "\n",
    "def fung(a,b,i):\n",
    "    if i > 0:\n",
    "        z = 6 * func(a,b,i -1) \n",
    "    else:\n",
    "        z =3\n",
    "    b[i] = z\n",
    "    return z\n",
    "\n",
    "func(a,b,5)\n",
    "\n",
    "print(a,b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
